import tensorflow as tf
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Dense, LSTM, Input, concatenate, Bidirectional, Multiply
from keras.optimizers import Adam
from keras.losses import BinaryCrossentropy
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
import csv, os, ast, sys, random
from keras.backend import variable
csv.field_size_limit(sys.maxsize) # Fixes _csv.Error: field larger than field limit


MAX_NUM_FEATURES = 75
MAX_FEATURE_LEN = 10
LSTM_UNITS = 50 # 16 Output shape doubles when using concatenate Bidirectional
NUM_EPOCHS = 50
BATCH_SIZE = 16
DROPOUT = 0.2
ONE_HOT_DEPTH = 49 # Equals number of APIs supported
ONE_HOT_FEATURE_LEN = MAX_FEATURE_LEN * ONE_HOT_DEPTH


# Takes a list of samples and splits it up for training and testing.
def split_samples(samples, percent_train):
    num_samples = len(samples)
    percent_train = int(percent_train * 10)
    training_samples = samples[:(num_samples // 10) * percent_train]
    testing_samples  = samples[(num_samples  // 10) * percent_train:]
    return training_samples, testing_samples


x_data_seq = list() # List of sequences for each report.
x_data_static = list() # List of counts for each sequence for each report.
y_data = list() # 0 (benign) or 1 (malign) for each report.

# Load data from output folder.
csv_files = os.listdir('output')
random.shuffle(csv_files)
for filename in csv_files:
    with open('output/' + filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        all_sequences = list() # All sequences for this report.
        all_counts = list()    # All sequence counts for this report.
        first_line = True
        csv_list = list(csv_reader)

        # Skip report if no sequences.
        if len(csv_list) <= 1:
            continue

        for line in csv_list:

            # First line contains 0 or 1.
            if first_line:
                y_data.append(int(line[0]))
                first_line = False

            # All other lines contain sequences.
            else:
                sequence = ast.literal_eval(line[0])
                count = int(line[1])
                all_sequences.append(sequence)
                all_counts.append(count)

        x_data_seq.append(all_sequences)
        x_data_static.append(all_counts)


for i in range(len(x_data_seq)):

    # Pad/truncate each sequence.
    x_data_seq[i] = pad_sequences(x_data_seq[i], maxlen=MAX_FEATURE_LEN, padding='post',
                                  truncating='post', value=0)

    # Pad/truncate number of sequences in each report.
    x_data_seq[i] = x_data_seq[i][:MAX_NUM_FEATURES]
    num_dummy_seq = MAX_NUM_FEATURES - len(x_data_seq[i])
    for _ in range(num_dummy_seq):
        x_data_seq[i] = np.append(x_data_seq[i], [[0] * MAX_FEATURE_LEN], axis=0)

# Pad/truncate sequence counts to match the padding/truncation we did with each sequence
x_data_static = pad_sequences(x_data_static, maxlen=MAX_NUM_FEATURES, padding='post',
                              truncating='post', value=0)

# Get number of samples.
num_samples = len(x_data_seq)

# One hot encode all the sequences.
x_data_seq = tf.one_hot(x_data_seq, depth=ONE_HOT_DEPTH, on_value=1, off_value=0, axis=-1).numpy()
x_temp = list()
for i in range(len(x_data_seq)):
    for j in range(len(x_data_seq[i])):
        test = np.concatenate(x_data_seq[i][j]) # Combine all sub arrays and append to temp array.
        x_temp.append(test)
x_data_seq = np.reshape(np.array(x_temp), (num_samples, MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))

# Split up data into 80% training and 20% test.
x_train, x_test = split_samples(x_data_seq, 0.8)
x_train_static, x_test_static = split_samples(x_data_static, 0.8)
y_train, y_test = split_samples(y_data, 0.8)

# Scale static input.
scaler = RobustScaler()
x_train_static = scaler.fit_transform(x_train_static)
x_test_static  = scaler.transform(x_test_static)

# Convert to correct data type.
# x_train = tf.cast(array(x_train), tf.float32)
# x_test  = tf.cast(array(x_test),  tf.float32)
# x_train_static = tf.cast(array(x_train_static), tf.float32)
# x_test_static  = tf.cast(array(x_test_static),  tf.float32)
# y_train = tf.cast(array(y_train), tf.float32)
# y_test  = tf.cast(array(y_test),  tf.float32)
x_train = np.array(x_train)
x_train_static = np.array(x_train)
y_train = np.array(y_train)

# Sequence input
seq_input = Input(shape=(MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))
seq_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, activation='relu', dropout=DROPOUT,
                              recurrent_dropout=DROPOUT), merge_mode='concat')(seq_input)

# Static input (number of times each sequence occurred).
static_input = Input(shape=(MAX_NUM_FEATURES, 1))
# Add DNN here?

# Combine sequence and static input.
merged = concatenate([seq_lstm, static_input], axis=-1) # Other ways?
merged = Dense(1, activation='sigmoid')(merged)

model = Model(inputs=[seq_input, static_input], outputs=merged)
model.summary()

opt = Adam(learning_rate=0.001) # Original: 0.001
bc = BinaryCrossentropy()
model.compile(optimizer=opt, loss=bc, metrics=['accuracy'])

model.fit(x=[x_train, x_train_static], y=y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=0)

# Evaluate the model
score = model.evaluate(x=[x_test, x_test_static], y=y_test, batch_size=BATCH_SIZE, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
