import tensorflow as tf
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Dense, LSTM, Input, Bidirectional, Dropout
from keras.optimizers import Adam, SGD
from keras.losses import BinaryCrossentropy
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
import csv, os, ast, sys, random
from keras.backend import variable
csv.field_size_limit(sys.maxsize) # Fixes _csv.Error: field larger than field limit


MAX_NUM_FEATURES = 75
MAX_FEATURE_LEN = 10
LSTM_UNITS = 50 # 16 Output shape doubles when using concatenate Bidirectional
NUM_EPOCHS = 100
BATCH_SIZE = 16
DROPOUT = 0.1
ONE_HOT_DEPTH = 49 # Equals number of APIs supported
ONE_HOT_FEATURE_LEN = MAX_FEATURE_LEN * ONE_HOT_DEPTH
VERBOSE = 1


# Takes a list of samples and splits it up for training and testing.
def split_samples(samples, percent_train):
    num_samples = len(samples)
    percent_train = int(percent_train * 10)
    training_samples = samples[:(num_samples // 10) * percent_train]
    testing_samples  = samples[(num_samples  // 10) * percent_train:]
    return training_samples, testing_samples


X_data_seq = list() # List of sequences for each report.
X_data_cnt = list() # List of counts for each sequence for each report.
y_data     = list() # 0 (benign) or 1 (malign) for each report.

# Load data from output folder.
csv_files = os.listdir('output')
random.shuffle(csv_files)
for filename in csv_files:
    with open('output/' + filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        all_sequences = list() # All sequences for this report.
        all_counts = list()    # All sequence counts for this report.
        first_line = True
        csv_list = list(csv_reader)

        # Skip report if no sequences.
        if len(csv_list) <= 1:
            continue

        for line in csv_list:

            # First line contains 0 or 1.
            if first_line:
                y_data.append(int(line[0]))
                first_line = False

            # All other lines contain sequences.
            else:
                sequence = ast.literal_eval(line[0])
                count = int(line[1])
                all_sequences.append(sequence)
                all_counts.append(count)

        X_data_seq.append(all_sequences)
        X_data_cnt.append(all_counts)


for i in range(len(X_data_seq)):

    # Pad/truncate each sequence.
    X_data_seq[i] = pad_sequences(X_data_seq[i], maxlen=MAX_FEATURE_LEN, padding='post',
                                  truncating='post', value=0)

    # Pad/truncate number of sequences in each report.
    X_data_seq[i] = X_data_seq[i][:MAX_NUM_FEATURES]
    num_dummy_seq = MAX_NUM_FEATURES - len(X_data_seq[i])
    for _ in range(num_dummy_seq):
        X_data_seq[i] = np.append(X_data_seq[i], [[0] * MAX_FEATURE_LEN], axis=0)

# Pad/truncate sequence counts to match the padding/truncation we did with each sequence
X_data_cnt = pad_sequences(X_data_cnt, maxlen=MAX_NUM_FEATURES, padding='post',
                              truncating='post', value=0)

# Get number of samples.
num_samples = len(X_data_seq)

# One hot encode all the sequences.
X_data_seq = tf.one_hot(X_data_seq, depth=ONE_HOT_DEPTH, on_value=1, off_value=0, axis=-1).numpy()
x_temp = list()
for i in range(len(X_data_seq)):
    for j in range(len(X_data_seq[i])):
        test = np.concatenate(X_data_seq[i][j]) # Combine all sub arrays and append to temp array.
        x_temp.append(test)
X_data_seq = np.reshape(np.array(x_temp), (num_samples, MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))

# Split up data into 80% training and 20% test.
X_train, X_test         = split_samples(X_data_seq, 0.8)
X_cnt_train, X_cnt_test = split_samples(X_data_cnt, 0.8)
y_train, y_test         = split_samples(y_data,     0.8)

# Scale static input.
scaler = MinMaxScaler()
X_cnt_train = scaler.fit_transform(X_cnt_train)
X_cnt_test  = scaler.transform(X_cnt_test)

# Append count to end of each sequence.
X_train = np.dstack((X_train, X_cnt_train))
X_test  = np.dstack((X_test,  X_cnt_test))

# Convert to correct data type.
y_train = np.array(y_train)
y_test = np.array(y_test)

input_layer = Input(shape=(MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN + 1))

lstm_layer = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, activation='relu', dropout=DROPOUT,
                           recurrent_dropout=DROPOUT), merge_mode='concat')(input_layer)
dense_layer = Dense(128, activation='relu')(lstm_layer)
dense_layer = Dropout(DROPOUT)(dense_layer)
dense_layer = Dense(1, activation='sigmoid')(dense_layer)

model = Model(inputs=input_layer, outputs=dense_layer)
model.summary()

opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=False, clipnorm=1.0)
bc = BinaryCrossentropy()
model.compile(optimizer=opt, loss=bc, metrics=['accuracy'])

model.fit(x=X_train, y=y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=VERBOSE)

# Evaluate the model
score = model.evaluate(x=X_test, y=y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
