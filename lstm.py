import tensorflow as tf
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Dense, LSTM, Flatten, Input, concatenate
from numpy import array, append
from sklearn.preprocessing import OneHotEncoder
from keras.backend import print_tensor
from keras.losses import BinaryCrossentropy
import csv, os, ast


MAX_NUM_FEATURES = 75
MAX_FEATURE_LEN = 10
LSTM_UNITS = 100
NUM_EPOCHS = 50
BATCH_SIZE = 16
OPTIMIZER = 'adam'
ONE_HOT_DEPTH = 50 # Also equals number of APIs supported
ONE_HOT_FEATURE_LEN = MAX_FEATURE_LEN * ONE_HOT_DEPTH


x_data_seq = list() # List of sequences for each report.
x_data_static = list() # List of counts for each sequence for each report.
y_data = list() # 0 (benign) or 1 (malign) for each report.

# Load data from output folder.
for filename in os.listdir('output'):
    with open('output/' + filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        all_sequences = list() # All sequences for this report.
        all_counts = list()    # All sequence counts for this report.
        first_line = True
        csv_list = list(csv_reader)

        # Skip report if no sequences.
        if len(csv_list) <= 1:
            continue

        for line in csv_list:

            # First line contains 0 or 1
            if first_line:
                y_data.append(int(line[0]))
                first_line = False

            # All other lines contain sequences
            else:
                sequence = ast.literal_eval(line[0])
                count = int(line[1])
                all_sequences.append(sequence)
                all_counts.append(count)

        x_data_seq.append(all_sequences)
        x_data_static.append(all_counts)


for i in range(len(x_data_seq)):

    # Pad/truncate each sequence.
    x_data_seq[i] = pad_sequences(x_data_seq[i], maxlen=MAX_FEATURE_LEN, padding='post', truncating='post', value=0)

    # Pad/truncate number of sequences in each report.
    x_data_seq[i] = x_data_seq[i][:MAX_NUM_FEATURES]
    num_dummy_seq = MAX_NUM_FEATURES - len(x_data_seq[i])
    for _ in range(num_dummy_seq):
        x_data_seq[i] = append(x_data_seq[i], [[0] * MAX_FEATURE_LEN], axis=0)

# Pad/truncate sequence counts to match the padding/truncation we did with each sequence
x_data_static = pad_sequences(x_data_static, maxlen=MAX_NUM_FEATURES, padding='post', truncating='post', value=0)

# Get number of samples.
num_samples = len(x_data_seq)

# One hot encode all the sequences
x_data_seq = tf.one_hot(x_data_seq, depth=ONE_HOT_DEPTH, on_value=1, off_value=0, axis=-1).numpy()
x_temp = list()
for i in range(len(x_data_seq)):
    for j in range(len(x_data_seq[i])):
        test = np.concatenate(x_data_seq[i][j]) # Combine all sub arrays and append to temp array.
        x_temp.append(test)
x_data_seq = np.reshape(np.array(x_temp), (num_samples, MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))

# Split up data into 80% training and 20% test.
x_train = x_data_seq[:(len(x_data_seq) // 10) * 8]
x_test  = x_data_seq[(len(x_data_seq)  // 10) * 8:]
x_train_static = x_data_static[:(len(x_data_static) // 10) * 8]
x_test_static  = x_data_static[(len(x_data_static)  // 10) * 8:]
y_train = y_data[:(len(y_data) // 10) * 8]
y_test  = y_data[(len(y_data)  // 10) * 8:]

# Convert to correct data type.
x_train = tf.cast(array(x_train), tf.float32)
x_test  = tf.cast(array(x_test),  tf.float32)
x_train_static = tf.cast(array(x_train_static), tf.float32)
x_test_static  = tf.cast(array(x_test_static),  tf.float32)
y_train = tf.cast(array(y_train), tf.float32)
y_test  = tf.cast(array(y_test),  tf.float32)

# Sequence input
seq_input = Input(shape=(MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))
seq_lstm = LSTM(LSTM_UNITS, activation='relu')(seq_input) # Return sequence? stateful?

# Static input (number of times each sequence occurred)
static_input = Input(shape=(MAX_NUM_FEATURES))

# Combine sequence and static input
merged = concatenate([seq_lstm, static_input], axis=-1)

merged = Dense(50, activation='relu')(merged)
merged = Dense(25, activation='relu')(merged)
merged = Dense(10, activation='relu')(merged)
merged = Dense(1, activation='sigmoid')(merged)

model = Model(inputs=[seq_input, static_input], outputs=merged)
model.summary()

model.compile(optimizer=OPTIMIZER, loss='binary_crossentropy', metrics=['accuracy'])

model.fit(x=[x_train, x_train_static], y=y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=1)

# Evaluate the model
model.evaluate([x_test, x_test_static], y_test, verbose=1)
