import ast
import csv
import multiprocessing as mp
import numpy as np
import os
import random
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # INFO messages are not printed.
import tensorflow as tf
from keras.layers import Bidirectional, Dense, Dropout, Flatten, Input, Layer, LayerNormalization, LSTM, Masking
from keras.losses import BinaryCrossentropy
from keras.models import Model, Sequential
from keras.optimizers import Adam, SGD
from matplotlib import pyplot as plt
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler

DATA_FILE = '/home/brandon/data.csv'
SEQS_PER_SAMPLE = 75
NUM_OS_RESOURCES = 6
NUM_ATTRIBUTES = 7
SEQ_LEN_OH = 1 + NUM_OS_RESOURCES + NUM_ATTRIBUTES + 29 * 10 # One-hot encoded.
ATTN_HEADS = 2 # Factors of 304: 1, 2, 4, 8, 16, 19, 38, 76, ...
FF_DIM = 32
EPOCHS = 32 # (64 also worked well)
DROPOUT = 0.4
BATCH_SIZE = 16
LR = 0.001
MASK_VALUE = -10
VERBOSE = 0
NUM_FOLDS = 10
OPT = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0)
#OPT = SGD(learning_rate=0.001, momentum=0.9, nesterov=False) # TODO: TRY
LOSS = BinaryCrossentropy()

class MultiHeadSelfAttention(Layer):
    def __init__(self, embed_dim, num_heads=8):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        if embed_dim % num_heads != 0:
            raise ValueError(
                'embedding dimension = %d should be divisible by number of heads = %d' % (embed_dim, num_heads)
            )
        self.projection_dim = embed_dim // num_heads
        self.query_dense    = Dense(embed_dim)
        self.key_dense      = Dense(embed_dim)
        self.value_dense    = Dense(embed_dim)
        self.combine_heads  = Dense(embed_dim)

    def attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        # x.shape = [batch_size, seq_len, embedding_dim]
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)
        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)
        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)
        query = self.separate_heads(
            query, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        key = self.separate_heads(
            key, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        value = self.separate_heads(
            value, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        attention, weights = self.attention(query, key, value)
        attention = tf.transpose(
            attention, perm=[0, 2, 1, 3]
        )  # (batch_size, seq_len, num_heads, projection_dim)
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.embed_dim)
        )  # (batch_size, seq_len, embed_dim)
        output = self.combine_heads(
            concat_attention
        )  # (batch_size, seq_len, embed_dim)
        return output


class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=DROPOUT):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadSelfAttention(embed_dim, num_heads)
        self.ffn = Sequential(
            [Dense(ff_dim, activation="relu"), Dense(embed_dim),]
        )
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)


def load_data(filename: str):
    X_data = list()
    y_data = list()
    with open(filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        sample_num = -1
        for line in csv_reader:
            if len(line) == 1:
                y_data.append(int(line[0]))
                sample_num += 1
            else:
                try:
                    X_data[sample_num].append(line)
                except:
                    X_data.append([line])
    return X_data, y_data


def get_model():
    input_layer = Input(shape=(SEQS_PER_SAMPLE, SEQ_LEN_OH))
    # mask_layer = Masking(mask_value=MASK_VALUE)(input_layer)
    transformer_layer = TransformerBlock(SEQ_LEN_OH, ATTN_HEADS, FF_DIM)(input_layer)
    flatten_layer = Flatten()(transformer_layer) # Try pooling
    dropout_layer = Dropout(DROPOUT)(flatten_layer)
    output_layer = Dense(1, activation='sigmoid')(dropout_layer)
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=OPT, loss=LOSS, metrics=['accuracy'])
    return model


def plot(history):
    if VERBOSE:
        plt.figure()
        plt.plot(history.history['accuracy'])
        plt.plot(history.history['val_accuracy'])
        plt.title('Model Accuracy')
        plt.ylabel('Accuracy')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Val'], loc='lower right')
        plt.figure()
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Val'], loc='upper right')
        plt.show()


accuracies = []
losses = []


def log_output(score):
    accuracies.append(score[0])
    losses.append(score[1])


def do_model(X_train, y_train, X_test, y_test):
    model = get_model()
    if VERBOSE:
        model.summary()
    model.fit(x=X_train, y=y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=VERBOSE)
    score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)
    loss = score[0]
    accuracy = score[1]
    return (accuracy, loss)


def main():
    X_data, y_data = load_data(DATA_FILE)

    # Pad number of sequences in each sample.
    for i in range(len(X_data)):
        num_dummy_seq = SEQS_PER_SAMPLE - len(X_data[i])
        for _ in range(num_dummy_seq):
            X_data[i].append([0] * SEQ_LEN_OH)


    X_data = np.asarray(X_data).astype('float32')
    y_data = np.asarray(y_data).astype('float32')

    pool = mp.Pool(mp.cpu_count()) # Use all CPU cores.

    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=3)
    for train_indices, test_indices in kf.split(X_data):
        X_train = X_data[train_indices]
        y_train = y_data[train_indices]
        X_test = X_data[test_indices]
        y_test = y_data[test_indices]
        
        # Scale counts.
        X_cnt_train = list()
        for sample in X_train:
            for seq in sample:
                count = seq[290]
                if count > 0:
                    X_cnt_train.append(count)
        X_cnt_train = np.reshape(X_cnt_train, (-1, 1))
        scaler = RobustScaler()
        X_cnt_train = scaler.fit_transform(X_cnt_train)
        tick = 0
        for i in range(len(X_train)):
            for j in range(len(X_train[i])):
                if X_train[i][j][290] > 0:
                    X_train[i][j][290] = X_cnt_train[tick]
                    tick += 1
        X_cnt_test = list()
        for sample in X_test:
            for seq in sample:
                count = seq[290]
                if count > 0:
                    X_cnt_test.append(count)
        X_cnt_test = np.reshape(X_cnt_test, (-1, 1))
        X_cnt_test = scaler.transform(X_cnt_test)
        tick = 0
        for i in range(len(X_test)):
            for j in range(len(X_test[i])):
                if X_test[i][j][290] > 0:
                    X_test[i][j][290] = X_cnt_test[tick]
                    tick += 1

        pool.apply_async(do_model, args=(X_train, y_train, X_test, y_test), callback=log_output)

    pool.close() # Close the pool for new tasks.
    pool.join() # Wait for all tasks to complete.


    print('Loss (all folds):', losses)
    print('mean = %f; var = %f; std = %f' % (np.mean(losses), np.var(losses), np.std(losses)))
    print('Accuracy (all folds):', accuracies)
    print('mean = %f; var = %f; std = %f' % (np.mean(accuracies), np.var(accuracies), np.std(accuracies)))

if __name__ == "__main__":
    main()

