import ast
import csv
import numpy as np
import os
import random
import sys
import tensorflow as tf
from keras.layers import Bidirectional, Dense, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Flatten, Input, Layer, LayerNormalization, Masking
from keras.losses import BinaryCrossentropy
from keras.models import Model, Sequential
from keras.optimizers import Adam, SGD
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler

# Fixes _csv.Error: field larger than field limit
csv.field_size_limit(sys.maxsize)

DATA_FOLDER = '/Volumes/Data/Malware_Samples/output/'
MAX_NUM_SEQ = 75
ATTENTION_HEADS = 3  # Number of attention heads
FF_DIMENSION = 16  # Hidden layer size in feed forward network inside transformer
NUM_EPOCHS = 32
BATCH_SIZE = 16
DROPOUT = 0.45
TRAIN_FRACTION = 0.8
ONE_HOT_DEPTH = 56
SEQUENCE_LEN = 10 * ONE_HOT_DEPTH
VERBOSE = 1

X_data_seq = list() # List of sequences for each report.
X_data_cnt = list() # List of counts for each sequence for each report.
y_data     = list() # Labels 0 (benign) or 1 (malign).

# Takes a list of samples and splits it up for training and testing.
def split_samples(samples, percent_train):
    num_samples = len(samples)
    percent_train = int(percent_train * 10)
    training_samples = samples[:(num_samples // 10) * percent_train]
    testing_samples  = samples[(num_samples  // 10) * percent_train:]
    return training_samples, testing_samples

class MultiHeadSelfAttention(Layer):
    def __init__(self, embed_dim, num_heads=8):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        if embed_dim % num_heads != 0:
            raise ValueError(
                f"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}"
            )
        self.projection_dim = embed_dim // num_heads
        self.query_dense    = Dense(embed_dim)
        self.key_dense      = Dense(embed_dim)
        self.value_dense    = Dense(embed_dim)
        self.combine_heads  = Dense(embed_dim)

    def attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        # x.shape = [batch_size, seq_len, embedding_dim]
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)
        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)
        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)
        query = self.separate_heads(
            query, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        key = self.separate_heads(
            key, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        value = self.separate_heads(
            value, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        attention, weights = self.attention(query, key, value)
        attention = tf.transpose(
            attention, perm=[0, 2, 1, 3]
        )  # (batch_size, seq_len, num_heads, projection_dim)
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.embed_dim)
        )  # (batch_size, seq_len, embed_dim)
        output = self.combine_heads(
            concat_attention
        )  # (batch_size, seq_len, embed_dim)
        return output


class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=DROPOUT):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadSelfAttention(embed_dim, num_heads)
        self.ffn = Sequential(
            [Dense(ff_dim, activation="relu"), Dense(embed_dim),]
        )
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# Load input data in random order.
for filename in random.sample(os.listdir(DATA_FOLDER), len(os.listdir(DATA_FOLDER))):
    with open(DATA_FOLDER + filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        try:
            csv_list = list(csv_reader)
        except:
            print('Skipping ' + filename)
            continue

        all_sequences = list() # All sequences for this report.
        all_counts = list()    # All sequence counts for this report.
        first_line = True

        for line in csv_list:
            # First line contains 0 or 1.
            if first_line:
                y_data.append(int(line[0]))
                first_line = False

            # All other lines contain sequences.
            else:
                sequence = ast.literal_eval(line[0]) # Convert string to list.
                count = int(line[1])
                all_sequences.append(sequence)
                all_counts.append(count)

        X_data_seq.append(all_sequences)
        X_data_cnt.append(all_counts)

# One hot encode API numbers.
num_samples = len(X_data_seq)
X_data_seq = tf.one_hot(X_data_seq, depth=ONE_HOT_DEPTH, on_value=1, off_value=0, axis=-1).numpy()
x_temp = list()
for i in range(len(X_data_seq)):
    for j in range(len(X_data_seq[i])):
        test = np.concatenate(X_data_seq[i][j]) # Combine all sub arrays and append to temp array.
        x_temp.append(test)
X_data_seq = np.reshape(np.array(x_temp), (num_samples, MAX_NUM_SEQ, SEQUENCE_LEN))

# Split up data into 80% training and 20% test.
X_train, X_test         = split_samples(X_data_seq, TRAIN_FRACTION)
X_cnt_train, X_cnt_test = split_samples(X_data_cnt, TRAIN_FRACTION)
y_train, y_test         = split_samples(y_data, TRAIN_FRACTION)

# Scale sequence counts (all, not just column-wise)
scaler = RobustScaler() # MaxAbsScaler, MinMaxScaler, RobustScaler
X_cnt_train = np.reshape(X_cnt_train, (len(X_train) * MAX_NUM_SEQ, 1))
X_cnt_train = scaler.fit_transform(X_cnt_train)
X_cnt_train = np.reshape(X_cnt_train, (len(X_train), MAX_NUM_SEQ))

X_cnt_test = np.reshape(X_cnt_test, (len(X_test) * MAX_NUM_SEQ, 1))
X_cnt_test = scaler.transform(X_cnt_test)
X_cnt_test = np.reshape(X_cnt_test, (len(X_test), MAX_NUM_SEQ))

# Append count to end of each sequence.
X_train = np.dstack((X_train, X_cnt_train))
X_test  = np.dstack((X_test, X_cnt_test))

# Convert to correct data type.
y_train = np.array(y_train)
y_test  = np.array(y_test)

input_layer = Input(shape=(MAX_NUM_SEQ, SEQUENCE_LEN + 1))
# mask_layer = Masking(mask_value=0)(input_layer)
transformer_layer = TransformerBlock(SEQUENCE_LEN + 1, ATTENTION_HEADS, FF_DIMENSION)(input_layer)
# pooling_layer = GlobalAveragePooling1D()(transformer_layer) # GlobalMaxPooling1D, Flatten
flatten_layer = Flatten()(transformer_layer)
dense_layer = Dropout(DROPOUT)(flatten_layer)
# dense_layer = Dense(64, activation='relu')(dense_layer)
# dense_layer = Dropout(DROPOUT)(dense_layer)
output_layer = Dense(1, activation='sigmoid')(dense_layer)

model = Model(inputs=input_layer, outputs=output_layer)
model.summary()

opt = Adam(learning_rate=0.001, epsilon=1e-09, amsgrad=False, clipnorm=1.0)
# opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=False, clipnorm=1.0)
bc = BinaryCrossentropy()
model.compile(opt, bc, metrics=['accuracy'])

history = model.fit(x=X_train, y=y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,
                    validation_data=(X_test, y_test), verbose=VERBOSE)

if VERBOSE:
    plt.figure()
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='lower right')

    plt.figure()
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'val'], loc='upper right')
    plt.show()

# Evaluate the model
# score = model.evaluate(x=X_test, y=y_test, batch_size=1024, verbose=VERBOSE)
# if not VERBOSE:
#     print('Test loss:', score[0])
#     print('Test accuracy:', score[1])
