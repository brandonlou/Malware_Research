import ast
import csv
import numpy as np
import os
import random
import tensorflow as tf
from keras.layers import Bidirectional, Dense, Dropout, Flatten, Input, Layer, LayerNormalization, LSTM, Masking
from keras.losses import BinaryCrossentropy
from keras.models import Model, Sequential
from keras.optimizers import Adam, SGD
from matplotlib import pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, StandardScaler


DATA_FILE = '/home/brandon/data.csv'
SEQS_PER_SAMPLE = 50
SEQ_LEN = 13
NUM_OS_RESOURCES = 2
NUM_ATTRIBUTES = 6
SEQ_LEN_OH = 1 + NUM_OS_RESOURCES + NUM_ATTRIBUTES + 20 * 10 # One-hot encoded.
TRAIN_FRAC = 0.8
ATTN_HEADS = 11
FF_DIM = 16
EPOCHS = 50
DROPOUT = 0.4
BATCH_SIZE = 8
LR = 0.001
MASK_VALUE = -10
VERBOSE = 1


class MultiHeadSelfAttention(Layer):
    def __init__(self, embed_dim, num_heads=8):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        if embed_dim % num_heads != 0:
            raise ValueError(
                'embedding dimension = %d should be divisible by number of heads = %d' % (embed_dim, num_heads)
            )
        self.projection_dim = embed_dim // num_heads
        self.query_dense    = Dense(embed_dim)
        self.key_dense      = Dense(embed_dim)
        self.value_dense    = Dense(embed_dim)
        self.combine_heads  = Dense(embed_dim)

    def attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        # x.shape = [batch_size, seq_len, embedding_dim]
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)
        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)
        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)
        query = self.separate_heads(
            query, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        key = self.separate_heads(
            key, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        value = self.separate_heads(
            value, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        attention, weights = self.attention(query, key, value)
        attention = tf.transpose(
            attention, perm=[0, 2, 1, 3]
        )  # (batch_size, seq_len, num_heads, projection_dim)
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.embed_dim)
        )  # (batch_size, seq_len, embed_dim)
        output = self.combine_heads(
            concat_attention
        )  # (batch_size, seq_len, embed_dim)
        return output


class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=DROPOUT):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadSelfAttention(embed_dim, num_heads)
        self.ffn = Sequential(
            [Dense(ff_dim, activation="relu"), Dense(embed_dim),]
        )
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)


def load_data(filename: str):
    X_data = list()
    y_data = list()
    with open(filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        for line in csv_reader:
            if len(line) == 1:
                y_data.append(int(line[0]))
            else:
                X_data.append([int(line[0]), int(line[1]), int(line[2])] + ast.literal_eval(line[3]))
    return np.reshape(X_data, (-1, SEQS_PER_SAMPLE, 13)), np.array(y_data)


# One hot encode sequence actions, resource, and attributes.
def one_hot_encode(X_data):
    num_samples = len(X_data)
    temp_X_data = list()
    for i in range(len(X_data)):
        for j in range(len(X_data[i])):
            count = X_data[i][j][0]
            resource = X_data[i][j][1]
            attribute = X_data[i][j][2]
            sequence = X_data[i][j][3:]

            one_hot_resource = [0] * NUM_OS_RESOURCES
            if resource > 0:
                one_hot_resource[resource - 1] = 1

            one_hot_attribute = [0] * NUM_ATTRIBUTES
            if attribute > 0:
                one_hot_attribute[attribute - 1] = 1

            one_hot_sequence = list()
            for k in sequence:
                one_hot_action = [0] * 20
                if k > 0:
                    one_hot_action[k - 1] = 1
                one_hot_sequence += one_hot_action
            temp_X_data += [count] + one_hot_resource + one_hot_attribute + one_hot_sequence
    X_data = np.reshape(temp_X_data, (num_samples, SEQS_PER_SAMPLE, SEQ_LEN_OH))
    return X_data


def get_model():
    input_layer = Input(shape=(SEQS_PER_SAMPLE, SEQ_LEN_OH))
    mask_layer = Masking(mask_value=MASK_VALUE)(input_layer)
    transformer_layer = TransformerBlock(SEQ_LEN_OH, ATTN_HEADS, FF_DIM)(mask_layer)
    flatten_layer = Flatten()(transformer_layer) # Try pooling
    dropout_layer = Dropout(DROPOUT)(flatten_layer)
    # TODO: Try more dense layers
    output_layer = Dense(1, activation='sigmoid')(dropout_layer)
    model = Model(inputs=input_layer, outputs=output_layer)
    return model


def plot(history):
    if VERBOSE:
        plt.figure()
        plt.plot(history.history['accuracy'])
        plt.plot(history.history['val_accuracy'])
        plt.title('Model Accuracy')
        plt.ylabel('Accuracy')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Val'], loc='lower right')

        plt.figure()
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Val'], loc='upper right')
        plt.show()


def main():
    X_data, y_data = load_data(DATA_FILE)
    X_data = one_hot_encode(X_data)
    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, train_size=TRAIN_FRAC)

    X_train = X_train.astype('float64')
    X_test = X_test.astype('float64')

    # Scale counts.
    #scaler = RobustScaler()
    #X_train[:,:,:1] = np.reshape(scaler.fit_transform(np.reshape(X_train[:,:,:1], (-1, 1))), (-1, SEQS_PER_SAMPLE, 1))
    #X_test[:,:,:1] = np.reshape(scaler.transform(np.reshape(X_test[:,:,:1], (-1, 1))), (-1, SEQS_PER_SAMPLE, 1))

    model = get_model()
    if VERBOSE:
        model.summary()

    opt = Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999)
    # opt = SGD(learning_rate=0.1, momentum=0.9, nesterov=True) # TODO: TRY
    bc = BinaryCrossentropy()
    model.compile(optimizer=opt, loss=bc, metrics=['accuracy'])

    history = model.fit(x=X_train, y=y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), verbose=VERBOSE)
    plot(history)


if __name__ == "__main__":
    main()

