import csv
import multiprocessing as mp
import os
import sys


def process_report(input_dir: str, filename: str, features):
    with open(input_dir + '/' + filename) as infile:
        try:
            lines = infile.readlines()
        except:
            print('Error: Cannot read', filename)
            return
    
    # Skip first line because it tells the verdict and isn't an API call.
    for i in range(1, len(lines)):
        lines[i] = lines[i].strip() # Remove the newline at the end of string.
        features[lines[i]] = None
    
    print('Read features:', filename)


def generate_feature(input_file: str, all_features: tuple, q):

    # Open preprocessed report file.
    with open(input_file) as infile:
        try:
            lines = infile.readlines()
        except:
            return
    
    # Strip off newline from each line.
    for i in range(len(lines)):
        lines[i] = lines[i].strip()
    
    verdict = str(lines[0])
    local_features = lines[1:]

    vector = list()
    for feature in all_features:
        if feature in local_features:
            vector.append('1')
        else:
            vector.append('0')
    
    q.put(''.join(vector) + ',' + verdict)
    print('Success:', input_file)



# Listens for messages on 'line' and writes to file.
def listener(output_file: str, q):
    with open(output_file, 'w') as f:
        while 1:
            m = q.get()
            if m == 'kill':
                break
            f.write(m + '\n')
            f.flush()


def main():
    if len(sys.argv) != 3:
        print('USAGE: %s [input directory] [output file]' % sys.argv[0])
        exit()

    input_dir = sys.argv[1]
    output_file = sys.argv[2]

    # Manages shared resources across processes.
    manager = mp.Manager()

    # Dictionary of all features (that can be shared among processes). We
    # actually treat this as a set, so all the keys are the features and their
    # values are None.
    feature_set = manager.dict()

    # For each preprocessed report, read its features and store them all in the
    # feature "set".
    pool = mp.Pool(mp.cpu_count())
    for filename in os.listdir(input_dir):
        pool.apply_async(process_report, (input_dir, filename, feature_set))
    pool.close()
    pool.join()

    # Convert the "set" to a tuple because it's ordered and immutable.
    feature_tuple = tuple(feature_set.keys())

    # Reset pool of jobs.
    q = manager.Queue()
    pool = mp.Pool(mp.cpu_count() + 1)

    # Put listener to work first.
    watcher = pool.apply_async(listener, (output_file, q))

    jobs = list()
    for filename in os.listdir(input_dir):
        job = pool.apply_async(generate_feature, (input_dir + '/' + filename, feature_tuple, q))
        jobs.append(job)
    
    # Collect results from the workers through the pool result queue.
    for job in jobs:
        job.get()
    
    # Now that we are done, kill the listener.
    q.put('kill')
    pool.close()
    pool.join()


if __name__ == "__main__":
    main()
