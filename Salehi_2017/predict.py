import os
import pickle
import sys
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score
from utils.colors import Colors

if len(sys.argv) != 4:
    print(f'Usage: python {sys.argv[0]} <input dir> <features file> <model file>')
    exit()

# Load top features to use.
sorted_features = pickle.load(open(sys.argv[2], 'rb'))
print(f'{Colors.YELLOW}Info: Loaded {sys.argv[2]}{Colors.ENDC}')

X_data = list()
y_data = list()

# Load all preprocessed files and build feature vectors.
input_dir = sys.argv[1]
for filename in os.listdir(input_dir):
    with open(f'{input_dir}/{filename}') as infile:
        try:
            lines = infile.read().splitlines()
        except:
            continue

    # Get the verdict of the report.
    verdict = lines[0]
    if verdict != '0' or verdict != '1':
        print(f'{Colors.RED}Error: First line of {filename} not 0 or 1{Colors.ENDC}')
        exit()

    vector = list()
    for feature in sorted_features:
        if feature in lines[1:]:
            vector.append(1)
        else:
            vector.append(0)

    X_data.append(vector)
    y_data.append(int(verdict))

# Load trained model from disk.
model = pickle.load(open(sys.argv[3], 'rb'))
y_pred = model.predict(X_data)

# Calculate metrics.
accuracy  = accuracy_score(y_data, y_pred)
precision = precision_score(y_data, y_pred)
recall    = recall_score(y_data, y_pred)
auc       = roc_auc_score(y_data, y_pred)
num_fp    = confusion_matrix(y_data, y_pred).ravel()[1]
f1        = f1_score(y_data, y_pred)

# Print performance.
print(f'Accuracy: {accuracy:.3f}')
print(f'Precision: {precision:.3f}')
print(f'Recall: {recall:.3f}')
print(f'AUC: {auc:.3f}')
print(f'Number false positives: {num_fp:.3f}')
print(f'F1 Score: {f1:.3f}')
