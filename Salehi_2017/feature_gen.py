import csv
import multiprocessing as mp
import os
import pickle
import sys
from utils.colors import Colors


def process_report(input_dir: str, filename: str, features):
    with open(f'{input_dir}/{filename}') as infile:
        try:
            lines = infile.readlines()
        except:
            print(f'{Colors.RED}Error: Cannot read {filename}{Colors.ENDC}')
            return

    # Skip first line because it tells the verdict and isn't an API call.
    for i in range(1, len(lines)):
        # Remove the newline at the end of string.
        features[lines[i].strip()] = None

    print(f'{Colors.YELLOW}Info: Read features from {filename}{Colors.ENDC}')


def generate_feature(input_file: str, all_features: tuple, q):

    # Open preprocessed report file.
    with open(input_file) as infile:
        try:
            lines = infile.readlines()
        except:
            return

    # Strip off newline from each line.
    for i in range(len(lines)):
        lines[i] = lines[i].strip()

    verdict = str(lines[0])
    local_features = lines[1:]

    vector = list() # Append to list instead of string because O(1).
    for feature in all_features:
        if feature in local_features:
            vector.append('1')
        else:
            vector.append('0')

    # Convert list of chars into a string.
    vector_str = ''.join(vector)

    q.put(f'{vector_str},{verdict}')
    print(f'{Colors.GREEN}Success: {input_file}{Colors.ENDC}')


# Listens for messages on 'line' and writes to file.
def listener(output_file: str, q):
    with open(output_file, 'w') as f:
        while 1:
            m = q.get()
            if m == 'kill':
                break
            f.write(f'{m}\n')
            f.flush()


def main():
    if len(sys.argv) != 3 or len(sys.argv) != 4:
        print(f'USAGE: {sys.argv[0]} <preprocessed dir> <output data file> [output features file]')
        exit()

    input_dir = sys.argv[1]
    output_file = sys.argv[2]

    # Manages shared resources across processes.
    manager = mp.Manager()

    # Dictionary of all features (that can be shared among processes). We
    # actually treat this as a set, so all the keys are the features and their
    # values are None.
    feature_set = manager.dict()

    # For each preprocessed report, read its features and store them all in the
    # feature "set".
    pool = mp.Pool(mp.cpu_count())
    for filename in os.listdir(input_dir):
        pool.apply_async(process_report, (input_dir, filename, feature_set))
    pool.close()
    pool.join()

    # Convert the "set" to a tuple because it's ordered and immutable.
    feature_tuple = tuple(feature_set.keys())

    # Save tuple of all features to use for prediction later.
    if len(sys.argv) == 4:
        pickle.dump(feature_tuple, open(sys.argv[3], 'wb'))
        print(f'Saved feature tuple to {sys.argv[3]}')

    # Reset pool of jobs.
    q = manager.Queue()
    pool = mp.Pool(mp.cpu_count() + 1)

    # Put listener to work first.
    pool.apply_async(listener, (output_file, q))

    jobs = list()
    for filename in os.listdir(input_dir):
        job = pool.apply_async(generate_feature, (f'{input_dir}/{filename}', feature_tuple, q))
        jobs.append(job)

    # Collect results from the workers through the pool result queue.
    for job in jobs:
        job.get()

    # Now that we are done, kill the listener.
    q.put('kill')
    pool.close()
    pool.join()


if __name__ == "__main__":
    main()
