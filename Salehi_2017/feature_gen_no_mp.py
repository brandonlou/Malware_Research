import csv
import os
import pickle
import sys
from utils.colors import Colors


data = list()

def process_report(input_dir: str, filename: str, features):
    with open(f'{input_dir}/{filename}', 'rt') as infile:
        try:
            lines = infile.readlines()
        except:
            print(f'{Colors.RED}Error: Cannot read {filename}{Colors.ENDC}')
            return

    # Skip first line because it tells the verdict and isn't an API call.
    for i in range(1, len(lines)):
        # Remove the newline at the end of string.
        features[lines[i].strip()] = None

    print(f'{Colors.YELLOW}Info: Read features from {filename}{Colors.ENDC}')


def generate_feature(input_file: str, all_features: tuple):

    # Open preprocessed report file.
    with open(input_file) as infile:
        try:
            lines = infile.readlines()
        except:
            return

    # Strip off newline from each line.
    for i in range(len(lines)):
        lines[i] = lines[i].strip()

    verdict = int(lines[0])
    local_features = lines[1:]

    vector = list()
    for feature in all_features:
        if feature in local_features:
            vector.append(1)
        else:
            vector.append(0)

    # Last element of the feature vector is 0 or 1 (benign/malign).
    vector.append(verdict)

    print(f'{Colors.GREEN}Success: {input_file}{Colors.ENDC}')
    return vector


def main():
    if len(sys.argv) not in (3, 4):
        print(f'USAGE: {sys.argv[0]} <preprocessed dir> <output data file> [output features file]')
        exit()

    input_dir = sys.argv[1]

    # Manages shared resources across processes.
    manager = mp.Manager()

    # Dictionary of all features (that can be shared among processes). We
    # actually treat this as a set, so all the keys are the features and their
    # values are None.
    feature_set = manager.dict()

    # For each preprocessed report, read its features and store them all in the
    # feature "set".
    with mp.get_context('spawn').Pool(mp.cpu_count()) as pool:
        for filename in os.listdir(input_dir):
            pool.apply_async(process_report, (input_dir, filename, feature_set))
        pool.close()
        pool.join()

    # Convert the "set" to a tuple because it's ordered and immutable.
    feature_tuple = tuple(feature_set.keys())

    # Save tuple of all features to use for prediction later.
    if len(sys.argv) == 4:
        with open(sys.argv[3], 'wb') as feature_file:
            pickle.dump(feature_tuple, feature_file, protocol=4)
            print(f'Saved feature tuple to {sys.argv[3]}')

    # Reset pool of jobs.
    with mp.get_context('spawn').Pool(mp.cpu_count()) as pool: 
        for filename in os.listdir(input_dir):
            pool.apply_async(generate_feature, (f'{input_dir}/{filename}', feature_tuple), callback=add_data)
        pool.close()
        pool.join()

    # Save data.
    with open(sys.argv[2], 'wb') as data_file:
        pickle.dump(data, data_file, protocol=4)
        print(f'Saved data to {sys.argv[2]}')


if __name__ == "__main__":
    main()
