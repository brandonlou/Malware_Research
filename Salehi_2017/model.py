import csv
import sys
from numpy import mean, std
from skfeature.function.similarity_based import fisher_score
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score
from sklearn import svm
from sklearn.feature_selection import RFE
from sklearn.pipeline import Pipeline
from utils.blr import EBLogisticRegression, VBLogisticRegression


FISHER_NUM_FEATURES = 1000
RFE_NUM_FEATURES = 380 # Number of features in our work.
NUM_FOLDS = 10


if len(sys.argv) != 2:
    print('USAGE: python3 %s [data file]' % sys.argv[0])
    exit()

X_data = list()
y_data = list()

# Get data from CSV file in the format (x, y).
with open(sys.argv[1], 'r') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    for row in csv_reader:
        vector = [int(char) for char in row[0]] # Parse bitstring to a list of 0's and 1's.
        verdict = int(row[1])
        X_data.append(vector)
        y_data.append(verdict)

### Fisher score: https://jundongl.github.io/scikit-feature/tutorial.html ###
score = fisher_score.fisher_score(X_data, y_data)

# Get the top features by their indices.
ranked_indices = fisher_score.feature_ranking(score)
ranked_indices = sorted(ranked_indices[:FISHER_NUM_FEATURES])

# Only include selected features in our data.
for i in range(len(X_data)):
    X_data[i] = [X_data[i][j] for j in ranked_indices]

# Apply SVM-RFE
rfe = RFE(estimator=svm.LinearSVC(), n_features_to_select=RFE_NUM_FEATURES)

# Classify using Bayesian logistic regression.
classifier = EBLogisticRegression()
pipeline = Pipeline(steps=[('s', rfe), ('c', classifier)])

# Do 10-fold cross validation two times to prevent overfitting.
cv = RepeatedStratifiedKFold(n_splits=NUM_FOLDS, n_repeats=2, random_state=1)
scores = cross_val_score(pipeline, X_data, y_data, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')

print('Mean Accuracy: %.3f' % mean(scores))
print('Standard Deviation: %.3f' % std(scores))
