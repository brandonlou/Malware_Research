import tensorflow as tf
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Dense, LSTM, Input, Bidirectional, Dropout
from keras.optimizers import Adam, SGD
from keras.losses import BinaryCrossentropy
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler
import csv, os, ast, sys, random
from keras.backend import variable
csv.field_size_limit(sys.maxsize) # Fixes _csv.Error: field larger than field limit

DATA_FOLDER = '/Volumes/Data/Malware_Samples/output/'
MAX_NUM_FEATURES = 75
MAX_FEATURE_LEN = 10
LSTM_UNITS = 16 # Output shape doubles when using concatenate Bidirectional
NUM_EPOCHS = 100
BATCH_SIZE = 16
DROPOUT = 0.1
ONE_HOT_DEPTH = 56 # Equals number of APIs supported
ONE_HOT_FEATURE_LEN = MAX_FEATURE_LEN * ONE_HOT_DEPTH
VERBOSE = 0


# Takes a list of samples and splits it up for training and testing.
def split_samples(samples, percent_train):
    num_samples = len(samples)
    percent_train = int(percent_train * 10)
    training_samples = samples[:(num_samples // 10) * percent_train]
    testing_samples  = samples[(num_samples  // 10) * percent_train:]
    return training_samples, testing_samples


X_data_seq = list() # List of sequences for each report.
X_data_cnt = list() # List of counts for each sequence for each report.
y_data     = list() # 0 (benign) or 1 (malign) for each report.

# Load data
for filename in random.sample(os.listdir(DATA_FOLDER), len(os.listdir(DATA_FOLDER))):
    with open(DATA_FOLDER + filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        all_sequences = list() # All sequences for this report.
        all_counts = list()    # All sequence counts for this report.
        first_line = True

        try:
            csv_list = list(csv_reader)
        except:
            print('Skipping ' + filename)
            continue

        for line in csv_list:

            # First line contains 0 or 1.
            if first_line:
                y_data.append(int(line[0]))
                first_line = False

            # All other lines contain sequences.
            else:
                sequence = ast.literal_eval(line[0])
                count = int(line[1])
                all_sequences.append(sequence)
                all_counts.append(count)

        X_data_seq.append(all_sequences)
        X_data_cnt.append(all_counts)


# Pad/truncate number of sequences in each report.
for i in range(len(X_data_seq)):
    X_data_seq[i] = X_data_seq[i][:MAX_NUM_FEATURES]
    num_dummy_seq = MAX_NUM_FEATURES - len(X_data_seq[i])
    for _ in range(num_dummy_seq):
        X_data_seq[i] = np.append(X_data_seq[i], [[0] * MAX_FEATURE_LEN], axis=0)

# Pad/truncate sequence counts to match the padding/truncation we did with each sequence
X_data_cnt = pad_sequences(X_data_cnt, maxlen=MAX_NUM_FEATURES, padding='post',
                           truncating='post', value=0)

# Get number of samples.
num_samples = len(X_data_seq)

# One hot encode all the sequences.
X_data_seq = tf.one_hot(X_data_seq, depth=ONE_HOT_DEPTH, on_value=1, off_value=0, axis=-1).numpy()
x_temp = list()
for i in range(len(X_data_seq)):
    for j in range(len(X_data_seq[i])):
        test = np.concatenate(X_data_seq[i][j]) # Combine all sub arrays and append to temp array.
        x_temp.append(test)
X_data_seq = np.reshape(np.array(x_temp), (num_samples, MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))

# Split up data into 80% training and 20% test.
X_train, X_test         = split_samples(X_data_seq, 0.8)
X_cnt_train, X_cnt_test = split_samples(X_data_cnt, 0.8)
y_train, y_test         = split_samples(y_data,     0.8)

# Scale static input.
scaler = StandardScaler()
X_cnt_train = scaler.fit_transform(X_cnt_train)
X_cnt_test  = scaler.transform(X_cnt_test)

# Convert to correct data type.
y_train = np.array(y_train)
y_test  = np.array(y_test)

# Multiply each sequence by its count (like weights?)
X_train = X_train.astype('float64')
X_test  = X_test.astype('float64')
for i in range(len(X_train)):
    for j in range(len(X_train[i])):
        X_train[i][j] *= X_cnt_train[i][j]
for i in range(len(X_test)):
    for j in range(len(X_test[i])):
        X_test[i][j] *= X_cnt_test[i][j]

# Append count to end of each sequence.
# X_train = np.dstack((X_train, X_cnt_train))
# X_test  = np.dstack((X_test,  X_cnt_test))

input_layer = Input(shape=(MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))

lstm_layer = Bidirectional(LSTM(LSTM_UNITS, return_sequences=False, activation='relu', dropout=DROPOUT,
                           recurrent_dropout=DROPOUT), merge_mode='concat')(input_layer)
dense_layer = Dense(128, activation='relu')(lstm_layer)
dense_layer = Dropout(DROPOUT)(dense_layer)
dense_layer = Dense(1, activation='sigmoid')(dense_layer)

model = Model(inputs=input_layer, outputs=dense_layer)
model.summary()

# opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True, clipnorm=0.25)
bc = BinaryCrossentropy()
model.compile(optimizer='adam', loss=bc, metrics=['accuracy'])

model.fit(x=X_train, y=y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=VERBOSE)

# Evaluate the model
score = model.evaluate(x=X_test, y=y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
