import csv
import os
import pickle
import sys
from collections import Counter
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score

if len(sys.argv) != 4:
    print('USAGE: python %s <preprocessed dir> <tf-idf file> <model file>' % sys.argv[0])
    exit()

feature_info = pickle.load(open(sys.argv[2], 'rb'))
sorted_scores = feature_info[0]
IDF = feature_info[1]
print('Info: Loaded %s.' % sys.argv[2])

X_data = list()
y_data = list()

# Go through each preprocessed report.
input_dir = sys.argv[1]
for filename in os.listdir(input_dir):
    with open(input_dir + '/' + filename) as infile:
        try:
            lines = infile.read().splitlines()
        except:
            continue

    # Get the verdict of the report.
    verdict = int(lines[0])
    if verdict not in (0, 1):
        print('Error: First line of %s not 0 or 1' % filename)
        exit()

    # Get API call that occurs most in this report.
    counter = Counter(lines[1:])
    max_occurance = counter.most_common()[0][1]

    # Build the TF-IDF vector.
    vector = list()
    for feature in sorted_scores:
        feature = feature[0] # We only want the API name.
        TF = counter[feature] / max_occurance
        vector.append(TF * IDF[feature])

    X_data.append(vector)
    y_data.append(verdict)

print('Info: Processed all files in %s.' % sys.argv[1])

# Load model and predict.
model = pickle.load(open(sys.argv[3], 'rb'))
y_pred = model.predict(X_data)

# Calculate metrics.
print('Accuracy:', accuracy_score(y_data, y_pred))
print('Precision:', precision_score(y_data, y_pred))
print('Recall:', recall_score(y_data, y_pred))
print('AUC:', roc_auc_score(y_data, y_pred))
print('Number false positives:', confusion_matrix(y_data, y_pred).ravel()[1])
print('F1 Score:', f1_score(y_data, y_pred))
