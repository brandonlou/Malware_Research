import csv
import os
import pickle
import sys
from collections import Counter
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, \
                            precision_score, recall_score, roc_auc_score
from utils.colors import Colors

if len(sys.argv) != 4:
    print(f'Usage: python {sys.argv[0]} <preprocessed dir> <tf-idf file> <model file>')
    exit()

feature_info = pickle.load(open(sys.argv[2], 'rb'))
sorted_scores = feature_info[0]
IDF = feature_info[1]
print(f'Info: Loaded {sys.argv[2]}')

X_data = list()
y_data = list()

# Go through each preprocessed report.
input_dir = sys.argv[1]
for filename in os.listdir(input_dir):
    with open(f'{input_dir}/{filename}') as infile:
        try:
            lines = infile.read().splitlines()
        except:
            continue

    # Get the verdict of the report.
    verdict = int(lines[0])
    if verdict not in (0, 1):
        print(f'{Colors.RED}Error: First line of {filename} not 0 or 1{Colors.ENDC}')
        exit()

    # Get API call that occurs most in this report.
    api_calls = lines[1:]
    if not api_calls: # Empty
        continue
    counter = Counter(api_calls)
    max_occurance = counter.most_common()[0][1]

    # Build the TF-IDF vector.
    vector = list()
    for feature in sorted_scores:
        feature = feature[0] # We only want the API name.
        TF = counter[feature] / max_occurance
        vector.append(TF * IDF[feature])

    X_data.append(vector)
    y_data.append(verdict)

print(f'Info: Processed all files in {sys.argv[1]}')

# Load model and predict.
model = pickle.load(open(sys.argv[3], 'rb'))
y_pred = model.predict(X_data)

# Calculate metrics.
accuracy = accuracy_score(y_data, y_pred)
precision = precision_score(y_data, y_pred)
recall = recall_score(y_data, y_pred)
auc = roc_auc_score(y_data, y_pred)
num_fp = confusion_matrix(y_data, y_pred).ravel()[1]
f1 = f1_score(y_data, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'AUC: {auc}')
print(f'Number false positives: {num_fp}')
print(f'F1 Score: {f1}')
