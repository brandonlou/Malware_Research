import csv
import os
import operator
import pickle
import sys
import math
from collections import Counter

NUM_FEATURES = 10 # Experiment with this value as desired.

# Relates a (feature, sample) to its occurance.
occurance = dict()

# Relates a sample to its feature that occurs most often.
max_occurance = dict()

# Relates a (feature, sample) to its TF value.
TF = dict()

# Relates a feature to its IDF value.
IDF_M = dict()
IDF_B = dict()

# Relates a (feature, sample) to its TF-IDF value.
TF_IDF_M = dict()
TF_IDF_B = dict()

num_occur_in_M = dict()
num_occur_in_B = dict()

# Takes a feature and all samples
def separability_analysis(feature: str, mal_samples, ben_samples, size_M: int, size_B: int, prior_M, prior_B):
    sum_M = 0
    sum_B = 0

    # Get number of malware samples that have this feature.
    if num_occur_in_M.get(feature) is None:
        num_occur_in_M[feature] = 1
        for sample in mal_samples:
            if feature in sample:
                num_occur_in_M[feature] += 1

    # Get number of benign samples that have this feature.
    if num_occur_in_B.get(feature) is None:
        num_occur_in_B[feature] = 1
        for sample in ben_samples:
            if feature in sample:
                num_occur_in_B[feature] += 1

    for sample in mal_samples:
        counter = Counter(sample)
        # Get how many times feature occurs in sample.
        occurance[(feature, sample)] = counter[feature]
        # Get maximum occuring feature in the sample.
        if max_occurance.get(sample) is None:
            max_occurance[sample] = counter.most_common()[0][1]
        TF[(feature, sample)] = occurance[(feature, sample)] / max_occurance[sample]
        if IDF_M.get(feature) is None:
            IDF_M[feature] = math.log(size_M / num_occur_in_M[feature])
        TF_IDF_M[(feature, sample)] = TF[(feature, sample)] * IDF_M[feature]
        sum_M += TF_IDF_M[(feature, sample)]

    mean_TF_IDF_M = sum_M / size_M

    for sample in ben_samples:
        counter = Counter(sample)
        # Get how many times feature occurs in sample.
        occurance[(feature, sample)] = counter[feature]
        # Get maximum occuring feature in the sample.
        if max_occurance.get(sample) is None:
            max_occurance[sample] = counter.most_common()[0][1]
        TF[(feature, sample)] = occurance[(feature, sample)] / max_occurance[sample]
        if IDF_B.get(feature) is None:
            IDF_B[feature] = math.log(size_B / num_occur_in_B[feature])
        TF_IDF_B[(feature, sample)] = TF[(feature, sample)] * IDF_B[feature]
        sum_B += TF_IDF_B[(feature, sample)]

    mean_TF_IDF_B = sum_B / size_B

    sum_M1 = 0
    for sample in mal_samples:
        sum_M1 += (TF_IDF_M[(feature, sample)] - mean_TF_IDF_M)**2

    # Variance of feature in Malware class.
    var_M = sum_M1 / size_M

    sum_B1 = 0
    for sample in ben_samples:
        sum_B1 += (TF_IDF_B[(feature, sample)] - mean_TF_IDF_B)**2

    # Variance of feature in Benign class.
    var_B = sum_B1 / size_B

    # Compute Within class variablity.
    var_within = (prior_M * var_M) + (prior_B * var_B)

    # Overall mean TF-IDF of feature.
    mean_TF_IDF = (sum_M + sum_B) / (size_M + size_B)

    # Compute between class variability as variance of class centers with
    # respect to global centers.
    var_btwn = ((mean_TF_IDF_M - mean_TF_IDF)**2 + (mean_TF_IDF_B - mean_TF_IDF)**2) / (size_M + size_B)

    # Compute total variability (the paper has a typo but I think this is correct)
    total_var = var_within + var_btwn

    # Compute separability score of the feature.
    score = total_var / var_within

    return score


def main():
    if len(sys.argv) not in (3, 4):
        print('USAGE: python %s <preprocessed dir> <output file> [tf-idf file]' % sys.argv[0])
        exit()

    input_dir = sys.argv[1]
    output_file = sys.argv[2]

    mal_samples = list()
    ben_samples = list()
    feature_set = set()

    for filename in os.listdir(input_dir):
        with open(input_dir + '/' + filename) as infile:
            try:
                lines = infile.read().splitlines()
            except:
                continue

        verdict = lines[0]
        if verdict == '0':
            ben_samples.append(tuple(lines[1:]))
        elif verdict == '1':
            sample = tuple(lines[1:])
            if sample:
                mal_samples.append(sample)
        else:
            print('Error: First line of %s not 0 or 1' % filename)
            exit()

        # Add features to set.
        for feature in lines[1:]:
            feature_set.add(feature)

        print('Info: Read %s features' % filename)

    # Convert list to tuple because hashable.
    mal_samples = tuple(mal_samples)
    ben_samples = tuple(ben_samples)

    # Get size of malign and benign feature set to avoid computing it many times in the future.
    size_M = len(mal_samples)
    size_B = len(ben_samples)
    size_C = size_M + size_B

    # Calculate prior probabilities
    prior_M = size_M / (size_M + size_B)
    prior_B = size_B / (size_M + size_B)

    # Do Scatter/Separability Assessment for feature selection.
    scores = dict()
    for feature in feature_set:
        scores[feature] = separability_analysis(feature, mal_samples, ben_samples, size_M, size_B, prior_M, prior_B)
        print('Info: Performed separability analysis on %s' % feature)
    sorted_scores = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)[:NUM_FEATURES]

    # Calculate IDF relative to the entire dataset.
    IDF = dict()
    for feature in sorted_scores:
        feature = feature[0]
        IDF[feature] = math.log(size_C / (num_occur_in_B[feature] + num_occur_in_M[feature]))

    # Delete output file since we're appending to it continuously.
    if os.path.exists(output_file):
        os.remove(output_file)

    # Make each element of feature vector the TF-IDF weight of a system call.
    for sample in mal_samples:
        vector = list()
        for feature in sorted_scores:
            feature = feature[0] # We only want the API name
            vector.append(TF[(feature, sample)] * IDF[feature])
        with open(output_file, 'at') as csv_file:
            csv_writer = csv.writer(csv_file)
            csv_writer.writerow([*vector, 1])
    for sample in ben_samples:
        vector = list()
        for feature in sorted_scores:
            feature = feature[0] # We only want the API name
            vector.append(TF[(feature, sample)] * IDF[feature])
        with open(output_file, 'at') as csv_file:
            csv_writer = csv.writer(csv_file)
            csv_writer.writerow([*vector, 0])

    # Save TF-IDF information to a file.
    if len(sys.argv) == 4:
        pickle.dump((sorted_scores, IDF), open(sys.argv[3], 'wb'))
        print('Saved TF-IDF information to %s.' % sys.argv[3])

    print('Success.')

if __name__ == "__main__":
    main()
