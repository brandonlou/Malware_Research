import os
os.environ['OMP_NUM_THREADS'] = '23'
os.environ['TF_NUM_INTRAOP_THREADS'] = '23'
os.environ['TF_NUM_INTEROP_THREADS'] = '23'
import gc
import pickle
import sys
import tensorflow as tf
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from tensorflow.keras import backend as k
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.callbacks import Callback


NUM_CORES = 23
tf.config.threading.set_intra_op_parallelism_threads(NUM_CORES)
tf.config.threading.set_inter_op_parallelism_threads(NUM_CORES)

NUM_FOLDS = 10
BATCH_SIZE = 100
EPOCHS = 50
LOSS = 'binary_crossentropy'
OPTIMIZER = 'adam'
NUM_CHUNKS = 1


def chunks(l, n):
    for i in range(n):
        yield l[i::n]


class ClearMemory(Callback):
    def on_epoch_end(self, epoch, logs=None):
        gc.collect()
        k.clear_session()


def main():
    if len(sys.argv) < 2:
        print(f'Usage: python3 {sys.argv[0]} <data pickle>')
        exit()

    # Load data
    X_data = []
    y_data = []
    with open(sys.argv[1], 'rb') as pickled_data:
        data = pickle.load(pickled_data)
        for row in data:
            X_data.append(row[1:])
            y_data.append(row[0])
    
    # Split data into 80% training, 20% testing
    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, shuffle=True, stratify=y_data)

    # Stacked autoencoder model
    input_dim = len(X_data[0])
    input_layer = Input(shape=(input_dim,))
    encode_input = Dense(input_dim, activation='relu')(input_layer)
    encode_6000 = Dense(6000, activation='relu')(encode_input)
    encode_2000 = Dense(2000, activation='relu')(encode_6000)
    encode_500 = Dense(500,  activation='relu')(encode_2000)
    decode_2000 = Dense(2000, activation='relu')(encode_500)
    decode_6000 = Dense(6000, activation='relu')(decode_2000)
    decode_input = Dense(input_dim, activation='relu')(decode_6000)
    model = Model(inputs=input_layer, outputs=decode_input)
    model.compile(loss=LOSS, optimizer=OPTIMIZER, run_eagerly=True)

    # Chunk data into batches to speed up fitting
    for chunk in chunks(X_train, NUM_CHUNKS):
        model.fit(chunk, chunk, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=ClearMemory())

    # Reduce to 500 features
    new_model = Model(inputs=input_layer, outputs=encode_500)
    new_model.compile(loss=LOSS, optimizer=OPTIMIZER, run_eagerly=True)
    X_train = new_model.predict(X_train)
    X_test = new_model.predict(X_test)

    # Train a decision tree
    dtc = DecisionTreeClassifier()
    dtc = dtc.fit(X_train, y_train)

    # Predict
    y_pred = dtc.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred)
    num_fp = confusion_matrix(y_test, y_pred).ravel()[1]
    f1 = f1_score(y_test, y_pred)

    print(f'Accuracy: {accuracy}')
    print(f'Precision: {precision}')
    print(f'Recall: {recall}')
    print(f'AUC: {auc}')
    print(f'False Positives: {num_fp}')
    print(f'F1 Score: {f1}')


if __name__ == "__main__":
    main()

