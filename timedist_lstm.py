import ast
import csv
import numpy as np
import os
import random
import sys
import tensorflow as tf
from keras.layers import Bidirectional, concatenate, Dense, Dropout, Input, Lambda, LSTM, TimeDistributed
from keras.losses import BinaryCrossentropy
from keras.models import Model, Sequential
from keras.optimizers import Adam, SGD
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler

# Fixes _csv.Error: field larger than field limit
csv.field_size_limit(sys.maxsize)

DATA_FOLDER = '/Volumes/Data/Malware_Samples/output/'
MAX_NUM_SEQUENCES = 75
MAX_TIMESTEPS = 10
LSTM_UNITS = 100
NUM_EPOCHS = 100
BATCH_SIZE = 16
DROPOUT = 0.1
ONE_HOT_DEPTH = 23 # API categories (6) + Max APIs per category (17)
VERBOSE = 1

# TODO: Fix one hot encoding to action then type (read then file)
def one_hot_encode(data):
    temp_list = list()
    for i in range(len(data)):
        for j in range(MAX_NUM_SEQUENCES):
            for k in range(MAX_TIMESTEPS):
                api_num = data[i][j][k]
                encoding = [0] * 23
                if api_num in range(1, 18):
                    encoding[0] = 1
                    encoding[api_num + 5] = 1
                elif api_num in range(17, 28):
                    encoding[1] = 1
                    encoding[api_num - 11] = 1
                elif api_num in range(28, 40):
                    encoding[2] = 1
                    encoding[api_num - 22] = 1
                elif api_num in range(40, 47):
                    encoding[3] = 1
                    encoding[api_num - 34] = 1
                elif api_num in range(47, 50):
                    encoding[4] = 1
                    encoding[api_num - 41] = 1
                elif api_num in range(50, 57):
                    encoding[5] = 1
                    encoding[api_num - 44] = 1
                temp_list.append(encoding)
    return np.reshape(temp_list, (len(data), MAX_NUM_SEQUENCES, MAX_TIMESTEPS, ONE_HOT_DEPTH))

# Takes a list of samples and splits it up for training and testing.
def split_samples(samples, percent_train):
    num_samples = len(samples)
    percent_train = int(percent_train * 10)
    training_samples = samples[:(num_samples // 10) * percent_train]
    testing_samples  = samples[(num_samples  // 10) * percent_train:]
    return training_samples, testing_samples


X_data_seq = list() # List of sequences for each report.
X_data_cnt = list() # List of counts for each sequence for each report.
y_data     = list() # Labels 0 (benign) or 1 (malign).

# Load input data in a random order
for filename in random.sample(os.listdir(DATA_FOLDER), len(os.listdir(DATA_FOLDER))):

    with open(DATA_FOLDER + filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        try:
            csv_list = list(csv_reader)
        except:
            print('Skipping ' + filename)
            continue

        all_sequences = list() # All sequences for this report.
        all_counts    = list() # All sequence counts for this report.
        first_line = True

        for line in csv_list:

            # First line contains 0 or 1.
            if first_line:
                y_data.append(int(line[0]))
                first_line = False

            # All other lines contain sequences.
            else:
                sequence = ast.literal_eval(line[0]) # Convert string to list.
                count = int(line[1])
                all_sequences.append(sequence)
                all_counts.append(count)

        X_data_seq.append(all_sequences)
        X_data_cnt.append(all_counts)

# One hot encode all the sequences
X_data_seq = one_hot_encode(X_data_seq)

# Split up data into 80% training and 20% test.
X_train, X_test         = split_samples(X_data_seq, 0.8)
X_cnt_train, X_cnt_test = split_samples(X_data_cnt, 0.8)
y_train, y_test         = split_samples(y_data, 0.8)

# Scale sequence counts (all, not just column-wise)
scaler = StandardScaler()
X_cnt_train = np.reshape(X_cnt_train, (320 * MAX_NUM_SEQUENCES, 1))
X_cnt_train = scaler.fit_transform(X_cnt_train)
X_cnt_train = np.reshape(X_cnt_train, (320, MAX_NUM_SEQUENCES))

X_cnt_test = np.reshape(X_cnt_test, (80 * MAX_NUM_SEQUENCES, 1))
X_cnt_test = scaler.transform(X_cnt_test)
X_cnt_test = np.reshape(X_cnt_test, (80, MAX_NUM_SEQUENCES))

# Append count to end of each sequence.
# X_train = np.dstack((X_train, X_cnt_train))
# X_test  = np.dstack((X_test,  X_cnt_test))

# Convert to correct data type.
y_train = np.array(y_train)
y_test  = np.array(y_test)

input1_layer = Input(shape=(MAX_NUM_SEQUENCES, MAX_TIMESTEPS, ONE_HOT_DEPTH))
lstm_layer = TimeDistributed(Bidirectional(LSTM(LSTM_UNITS, return_sequences=False, activation='relu', dropout=DROPOUT, recurrent_dropout=DROPOUT)))(input1_layer)
lstm_layer = TimeDistributed(Dense(128, activation='relu'))(lstm_layer)

input2_layer = Input(shape=(MAX_NUM_SEQUENCES, 1))
dense_layer = Dense(128, activation='relu')(input2_layer)

concat_layer = concatenate([lstm_layer, input2_layer], axis=-1)
dense_layer = Dense(128, activation='relu')(concat_layer)
output_layer = Dense(1, activation='sigmoid')(dense_layer)

# input_layer = Input(shape=(MAX_NUM_SEQUENCES, ONE_HOT_FEATURE_LEN + 1))
# lstm_layers = list()
# for i in range(MAX_NUM_SEQUENCES):
#     print(input_layer.shape)
#     split_layer = Lambda(lambda x: x[:, i])(input_layer)
#     print(split_layer.shape)
#     lstm_layer = Bidirectional(
#                     LSTM(LSTM_UNITS, return_sequences=True, activation='relu', dropout=DROPOUT, recurrent_dropout=DROPOUT),
#                     merge_mode='concat')(split_layer)
#     lstm_layers.append(lstm_layer)
# merged = concatenate(lstm_layers)
# merged = Dense(1, activation='sigmoid')(merged)

model = Model(inputs=[input1_layer, input2_layer], outputs=output_layer)
model.summary()

# opt = SGD(learning_rate=0.01, momentum=0.95, nesterov=False, clipnorm=1.0)
bc = BinaryCrossentropy()
model.compile(optimizer='adam', loss=bc, metrics=['accuracy'])

model.fit(x=[X_train, X_cnt_train], y=y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=VERBOSE)

# Evaluate the model
score = model.evaluate(x=[X_test, X_cnt_test], y=y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)
if not VERBOSE:
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
