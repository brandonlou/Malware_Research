import ast
import csv
import numpy as np
import os
import random
import sys
import tensorflow as tf
from keras.layers import Bidirectional, concatenate, Dense, Dropout, Input, LSTM
from keras.losses import BinaryCrossentropy
from keras.models import Model
from keras.optimizers import Adam, SGD
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler

# Fixes _csv.Error: field larger than field limit
csv.field_size_limit(sys.maxsize)

DATA_FOLDER = '/Volumes/Data/Malware_Samples/output/'
MAX_NUM_FEATURES = 75
MAX_FEATURE_LEN = 10
LSTM_UNITS = 16
NUM_EPOCHS = 100
BATCH_SIZE = 16
DROPOUT = 0.2
ONE_HOT_DEPTH = 56 # Number of APIs supported.
ONE_HOT_FEATURE_LEN = MAX_FEATURE_LEN * ONE_HOT_DEPTH
VERBOSE = 0


# Takes a list of samples and splits it up for training and testing.
def split_samples(samples, percent_train):
    num_samples = len(samples)
    percent_train = int(percent_train * 10)
    training_samples = samples[:(num_samples // 10) * percent_train]
    testing_samples  = samples[(num_samples  // 10) * percent_train:]
    return training_samples, testing_samples


X_data_seq = list() # List of sequences for each report.
X_data_cnt = list() # List of counts for each sequence for each report.
y_data     = list() # Labels 0 (benign) or 1 (malign).

# Load data.
for filename in random.sample(os.listdir(DATA_FOLDER), len(os.listdir(DATA_FOLDER))):
    with open(DATA_FOLDER + filename, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        all_sequences = list() # All sequences for this report.
        all_counts = list()    # All sequence counts for this report.
        first_line = True

        try:
            csv_list = list(csv_reader)
        except:
            print('Skipping ' + filename)
            continue

        for line in csv_list:

            # First line contains 0 or 1.
            if first_line:
                y_data.append(int(line[0]))
                first_line = False

            # All other lines contain sequences.
            else:
                sequence = ast.literal_eval(line[0]) # Convert string to list.
                count = int(line[1])
                all_sequences.append(sequence)
                all_counts.append(count)

        X_data_seq.append(all_sequences)
        X_data_cnt.append(all_counts)

# Pad/truncate number of sequences in each report.
for i in range(len(X_data_seq)):
    X_data_seq[i] = X_data_seq[i][:MAX_NUM_FEATURES]
    num_dummy_seq = MAX_NUM_FEATURES - len(X_data_seq[i])
    for _ in range(num_dummy_seq):
        X_data_seq[i] = np.append(X_data_seq[i], [[0] * MAX_FEATURE_LEN], axis=0)

# Pad/truncate sequence counts to match the padding/truncation we did with each sequence
X_data_cnt = pad_sequences(X_data_cnt, maxlen=MAX_NUM_FEATURES, padding='post',
                              truncating='post', value=0)

# Get number of samples.
num_samples = len(X_data_seq)

# One hot encode all the sequences.
X_data_seq = tf.one_hot(X_data_seq, depth=ONE_HOT_DEPTH, on_value=1, off_value=0, axis=-1).numpy()
x_temp = list()
for i in range(len(X_data_seq)):
    for j in range(len(X_data_seq[i])):
        test = np.concatenate(X_data_seq[i][j]) # Combine all sub arrays and append to temp array.
        x_temp.append(test)
X_data_seq = np.reshape(np.array(x_temp), (num_samples, MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))

# Split up data into 80% training and 20% test.
X_train, X_test         = split_samples(X_data_seq, 0.8)
X_cnt_train, X_cnt_test = split_samples(X_data_cnt, 0.8)
y_train, y_test         = split_samples(y_data, 0.8)

# Scale static input.
scaler = RobustScaler()
X_cnt_train = scaler.fit_transform(X_cnt_train)
X_cnt_test  = scaler.transform(X_cnt_test)

# Convert to correct data type.
y_train = np.array(y_train)
y_test  = np.array(y_test)

# Sequence input
seq_input = Input(shape=(MAX_NUM_FEATURES, ONE_HOT_FEATURE_LEN))
seq_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, activation='relu', dropout=DROPOUT,
                              recurrent_dropout=DROPOUT), merge_mode='concat')(seq_input)

# Static input (number of times each sequence occurred).
static_input = Input(shape=(MAX_NUM_FEATURES, 1))
static_dense = Dense(128, activation='relu')(static_input)

# Combine sequence and static input.
merged = concatenate([seq_lstm, static_dense], axis=-1) # Other ways?
merged = Dense(1, activation='sigmoid')(merged)

model = Model(inputs=[seq_input, static_input], outputs=merged)
model.summary()

opt = SGD(learning_rate=0.01, momentum=0.95, nesterov=False, clipnorm=1.0)
bc = BinaryCrossentropy()
model.compile(optimizer=opt, loss=bc, metrics=['accuracy'])

model.fit(x=[X_train, X_cnt_train], y=y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=VERBOSE)

# Evaluate the model
score = model.evaluate(x=[X_test, X_cnt_test], y=y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
